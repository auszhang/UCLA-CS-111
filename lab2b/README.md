NAME: Christopher Aziz
EMAIL: caziz@ucla.edu

# Project 2B: Lock Granularity and Performance

## Included Files

File            | Details
--------------- | -------
SortedList.h    | Header file for doubly linked sorted list
SortedList.c    | Source code for doubly linked sorted list that implements insert, delete, lookup, and length methods
lab2_list.c     | Source code for program that uses sorted list in a multithread environment
lab2_test.sh    | Shell script used to test the list program and the generate .csv file
lab2b_1.png     | Image of "Throughput vs. Number of Threads for Synchronized List Operations"
lab2b_2.png     | Image of "Average Time Per Op/Mutex Lock vs. Threads for Mutex List Ops"
lab2b_3.png     | Image of "Successful Iterations vs. Threads for List Operations"
lab2b_4.png     | Image of "Throughput vs. Number of Threads for Mutex Sub-Lists"
lab2b_5.png     | Image of "Throughput vs. Number of Threads for Spin-lock Sub-lists"
lab2_list.csv   | Comma separated values file for `lab2_list.c` generated by `lab2_test.sh` and used by `lab2_list.gp`
lab2_list.gp    | GNU plot script to create `.png` images from the `lab2_list.csv` output
Makefile        | Makefile that includes the targets `build`, `clean`, `tests` to run tests and generate the .csv file, `graphs` to run scripts to generate the graph images, `profile`, to generate profile.out, and `dist`
README          | This file which describes each of the included files, has answers to the questions, and includes other information about the submission

## Questions

### 2.3.1 - Cycles in the basic list implementation
Where do you believe most of the cycles are spent in the 1 and 2-thread list tests?
Why do you believe these to be the most expensive parts of the code?

        When there is one thread, most of the cycles are spent on the operations because the single thread always has access to the locks and does not spend many cycles before it is able to begin operations.
        When there are two threads, there is at most one thread doing operations at a time.
        For two threads and spin locks, locking takes up most of the cycles because one thread is spinning while the other is doing operations, plus there is the overhead of setting up the locks.
        For two threads and mutex locks, it is likely that operations take up most of the cycles if we assume that the mutex locks are efficient and can put the second thread to sleep and wake it up without much overhead.

Where do you believe most of the time/cycles are being spent in the high-thread spin-lock tests?
Where do you believe most of the time/cycles are being spent in the high-thread mutex tests?

        For high-thread spin-lock tests, most of the CPU cycles are spent on spinning because most of the threads are always spinning while one is doing operations.
        For high-thread mutex tests, it is more likely that most of the CPU cycles are spent on operations. However, context-switching between threads is expensive and tests with short list operations may spend more time on this context-switching.

### 2.3.2 - Execution Profiling:

Where (what lines of code) are consuming most of the cycles when the spin-lock version of the list exerciser is run with a large number of threads?

        After using the profile on the spin-lock version, I found that the sync() function consumes most of the cycles. This makes sense because this is the function that determines and implements the locking mechanism for the list. The line in this function that consumes the most cycles is the `while (__sync_lock_test_and_set(&spin_lock, 1));` which spins until the lock is freed.

Why does this operation become so expensive with large numbers of threads?

        With a large number a threads, only one thread can be doing work on the list at any time. This means that the other threads spend their entire time slice spinning, consuming lots of cycles.

### 2.3.3 - Mutex Wait Time:

Look at the average time per operation (vs. # threads) and the average wait-for-mutex time (vs. #threads).
Why does the average lock-wait time rise so dramatically with the number of contending threads?

        The average lock-wait time rises dramatically because as the number of threads increase at most one thread can work at a time. This means that threads spend a longer time waiting for the lock as the number of contending threads increase.

Why does the completion time per operation rise (less dramatically) with the number of contending threads?

        As there are more threads, the overhead of putting threads to sleep and waking them up increases, increasing the overall time per operation. More significantly, as there are more threads, the odds that a previous thread added elements to the list without deleting them increases. Thus, subsequent threads must iterate through other threads' elements in order to complete their operations. This less dramatically increases the completion time per operation.

How is it possible for the wait time per operation to go up faster (or higher) than the completion time per operation?

        The completion time is the  average amount of time per operation, but the wait time is the average amount of time per operation for an individual thread. Since the wait times for threads often overlap with each other, the average wait time is often higher (increases faster) than the actual amount of time it takes to complete an operation.


### 2.3.4 - Performance of Partitioned Lists
Explain the change in performance of the synchronized methods as a function of the number of lists.

        As the number of lists increase, the performance (throughput) increases. This makes sense without many lists, the contention for the lists by each thread is high. With more lists, the resource is split between the threads, decreasing contention. Thus, threads are able to work concurrently on their independent sub-lists.

Should the throughput continue increasing as the number of lists is further increased? If not, explain why not.

        The throughput will continue increasing until the overhead cost of creating more lists and locks is greater than the benefit of reducing contention between the threads. In addition, there is a decreasing benefit to increasing the number of lists as after a certain point the liklihood that too threads will use the same sub-list is very close to 0. Thus, there will certainly be a point in which the throughput does not increase as the number of lists is further increased.

It seems reasonable to suggest the throughput of an N-way partitioned list should be equivalent to the throughput of a single list with fewer (1/N) threads. Does this appear to be true in the above curves? If not, explain why not.

        No, this is not true because dividing the list into sub-lists shortens the the traversal length of each list and thus reduces the number of time holding the locks. This reduces the chance of contention and increases the relative throughput for the partitioned list.
